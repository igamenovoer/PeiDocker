# InvokeAI GPU-enabled container with multi-user support
# This example creates a GPU-capable container with InvokeAI pre-installed
# Supports multiple concurrent users with separate GPU assignments

stage_1:
  # Base image configuration - using NVIDIA CUDA runtime
  image:
    base: nvidia/cuda:12.1.1-runtime-ubuntu22.04  # CUDA 12.1.1 runtime on Ubuntu 22.04
    output: invoke-ai-pip:stage-1                  # Name of the generated stage-1 image

  # SSH server configuration
  ssh:
    enable: true                    # Enable SSH server in the container
    port: 22                       # SSH port inside the container
    host_port: 3333                # Port on host machine mapped to container SSH port (different from default 2222)

    # Define SSH users with their passwords
    users:
      me:
        password: '123456'          # Regular user for development
      root:
        password: 'root'            # Root user for admin tasks

  # Proxy configuration for networks with restrictive policies
  proxy:
    address: host.docker.internal  # Proxy address (maps to host machine)
    port: 30080                    # Proxy port on host machine
    enable_globally: true          # Enable proxy for all shell commands during build and run
    remove_after_build: false     # Keep proxy settings after build for runtime use

  # GPU support configuration
  device:
    type: gpu                      # Enable GPU support (requires NVIDIA Docker runtime)

  # Package repository configuration for faster downloads
  apt:
    # Alternative repositories for different regions (uncomment as needed):
    # repo_source: 'stage-1/system/apt/ubuntu-22.04-tsinghua-x64.list'
    # Available options: 'tuna', 'aliyun', '163', 'ustc', 'cn'
    repo_source: 'tuna'            # Use Tsinghua University mirror (good for China/Asia)
    keep_repo_after_build: true    # Keep the apt source file after build
    use_proxy: false               # Don't use proxy for apt (direct connection)
    keep_proxy_after_build: false  # Don't keep proxy settings for apt after build

  # Custom scripts configuration for stage-1
  custom:
    on_build:                      # Scripts to run during image building
      # Run twice to ensure installation is successful (handles potential network issues)
      - 'stage-1/system/invoke-ai/install-invoke-ai-deps.sh'
      - 'stage-1/system/invoke-ai/install-invoke-ai-deps.sh'

# Stage 2: InvokeAI application layer with multi-user configuration
stage_2:
  # Image configuration
  image:
    output: invoke-ai-pip:stage-2  # Name of the final stage-2 image

  # Port mapping for InvokeAI web interfaces (multiple users)
  ports:
    - '9090-9099:9090-9099'        # Map port range for multiple InvokeAI instances

  # GPU support (inherited from stage-1)
  device:
    type: gpu                      # Maintain GPU support in final image

  # InvokeAI-specific environment variables
  environment:
    - 'AI_USERS=user_9090,user_9091,user_9092'     # User names for each InvokeAI instance
    - 'AI_PORTS=9090,9091,9092'                    # Ports for each user's InvokeAI instance
    - 'AI_DEVICES=cuda:0,cuda:1,cuda:2'            # GPU device assignment for each user (NO SPACES!)
    - 'AI_DATA_DIR=/invokeai-data'                 # Data storage directory inside container
    - 'INVOKEAI_ROOT=/soft/app/invokeai'           # InvokeAI installation directory
    - 'INVOKEAI_RAM=12'                            # CPU memory cache per user (GB) - larger = faster
    - 'INVOKEAI_VRAM=2'                            # GPU memory cache per user (GB) - larger = faster
    - 'INVOKEAI_AUTO_START=1'                      # Automatically start InvokeAI on container startup

  # Persistent storage configuration using auto-managed Docker volumes
  storage:
    app:                           # Directory for installed applications (including InvokeAI)
      type: auto-volume            # Use automatically managed Docker volume
    data:                          # Directory for general data files
      type: auto-volume            # Use automatically managed Docker volume
    workspace:                     # Directory for workspace/code
      type: auto-volume            # Use automatically managed Docker volume

  # Additional volume mounts for InvokeAI-specific directories
  mount:
    home-root:                     # Root user home directory
      type: auto-volume            # Use automatically managed Docker volume
      dst_path: /root              # Mount point in container
    invokeai-data:                 # InvokeAI user data and generated images
      type: auto-volume            # Use automatically managed Docker volume
      dst_path: /invokeai-data     # Mount point matching AI_DATA_DIR environment variable
    models:                        # External models directory (for custom/downloaded models)
      type: auto-volume            # Use automatically managed Docker volume
      dst_path: /models            # Mount point in container
    pei_init:                      # PeiDocker initialization scripts
      type: auto-volume            # Use automatically managed Docker volume
      dst_path: /pei-init          # Mount point in container
    pei_from_host:                 # Host scripts (allows modification without rebuilding image)
      type: auto-volume            # Use automatically managed Docker volume
      dst_path: /pei-from-host     # Mount point in container
  
  # Custom scripts configuration for stage-2
  custom:
    on_first_run:                  # Scripts to run only on first container startup
      - stage-2/system/conda/auto-install-miniconda.sh    # Install Miniconda first
      - stage-2/system/invoke-ai/install-invoke-ai-conda.sh  # Install InvokeAI via conda

    on_every_run:                  # Scripts to run on every container startup
      # Automatically start InvokeAI instances (controlled by INVOKEAI_AUTO_START)
      - stage-2/system/invoke-ai/invoke-ai-entrypoint.sh

# Note: This configuration supports multiple concurrent InvokeAI users
# Each user gets their own GPU device and dedicated resources
# Web interfaces will be available at:
# - http://localhost:9090 (user_9090)
# - http://localhost:9091 (user_9091)
# - http://localhost:9092 (user_9092)
# Adjust AI_USERS, AI_PORTS, and AI_DEVICES as needed for your setup